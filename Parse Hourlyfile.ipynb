{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Network Usage Statistics by MAC\n",
    "Get detatiled and fairly accurate usage statistics for every device on your network using the version of bwmon here: https://github.com/vortex-5/ddwrt-bwmon. This is a nice little collection of scripts that interrogates the network and saves necessary information in a simple way. I wanted to be able to see changes over time, so developed this quick and dirty python program and underlying manual process to keep things running. There is still a lot that I can do to automate this, most notably using another machine as the controller - probably a RPi as it used to be before being repurposed for ADSB. \n",
    "\n",
    "The README.md file has more information on the hardware, but I am using a Buffalo router running DD-WRT that does not have permamanent memory. So if the router goes down there is a process to reload and restart the process. ORiginally, this was an automatic process fired off by an RPi watching on SCP, but currently it is manual. \n",
    "\n",
    "If the router goes down, you have to do this to restore BWMON:\n",
    "```\n",
    "scp -i ~/.ssh/router -r ~/Documents/repositories/Router-Tools/bwmon root@192.168.XXX.XXX:/var\n",
    "scp -i ~/.ssh/router mac-names.txt root@192.168.XXX.XXX:/tmp/var/bwmon/www/mac-names.js\n",
    "```\n",
    "\n",
    "Here are some handy SCPs for the manual part of the process.\n",
    "\n",
    "Backup the bwmon directory:\n",
    "```\n",
    "scp -i ~/.ssh/router -r root@192.168.XXX.XXX:/var/bwmon /Users/samuelcroker/Documents/repositories/Router-Tools/\n",
    "```\n",
    "Note, the bwmon directory\n",
    "```\n",
    "ssh -i ~/.ssh/router root@192.168.XXX.XXX\n",
    "/var/bwmon/start.sh\n",
    "```\n",
    "The cron jobs are saved in the dd-wrt app, which appears not to be volitile. We should check that out sometime.\n",
    "\n",
    "Grab the daily updates:\n",
    "```\n",
    "scp -i ~/.ssh/router root@192.168.XXX.XXX:/var/bwmon/data/*.dat ~/Documents/routerdata\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from os import walk\n",
    "import calmap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import getpass as gp\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Note:\n",
    "The history is maintained for all time in the second folder listed above. The router will keep all copies up until it loses power, then they are lost. For that reason, it is essential to maintain another copy. In the past, I have used a slave RPi to perform this function, but it was needed for another project so that fell by the wayside. In the current configuration I am using another computer but this is a manual process. Obviously there are lots of optimizations that can happen to fully develop this concept, but it is good enough for what it was designed to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router Admin IP:········\n",
      "Extracting hourly usage data\n"
     ]
    }
   ],
   "source": [
    "#Get user input - router ip address\n",
    "router_ip = gp.getpass(\"Router Admin IP:\")\n",
    "print(\"Extracting hourly usage data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved data from router\n",
      "CPU times: user 2.73 ms, sys: 6.35 ms, total: 9.07 ms\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#update the mac list - this will only run under Macbook\n",
    "#os.system(\"scp -i ~/.ssh/router mac-names.txt root@192.168.11.1:/tmp/var/bwmon/www/mac-names.js\")\n",
    "# move the dailyfiles - look for a 0 return code\n",
    "\n",
    "retcd = os.system(\"scp -i ~/.ssh/router root@\" + router_ip +\":/var/bwmon/data/*.dat ~/Documents/routerdata\")\n",
    "if retcd ==0:\n",
    "    print(\"Successfully retrieved data from router\")\n",
    "else:\n",
    "    print(\"An error occured in data transfer from router. ERR=\",str(retcd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.38 ms, sys: 6.37 ms, total: 15.8 ms\n",
      "Wall time: 17.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maccsv = '/Users/samuelcroker/Documents/repositories/Router-tools/mac-names.csv'\n",
    "macdf = pd.read_csv(maccsv, names = ['mac','devicename'],quotechar=\"'\")\n",
    "hourlypath = '/Users/samuelcroker/Documents/routerdata/'\n",
    "hourlyfiles = []\n",
    "for (dirpath, dirnames, filenames) in walk(hourlypath):\n",
    "    hourlyfiles.extend(filenames)\n",
    "    break\n",
    "macdf['MAC'] = macdf[str.lower('mac')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cutoff = 1.5\n",
    "idx = 0\n",
    "dailypath = '/Users/samuelcroker/Documents/routerdata/'\n",
    "dailyfiles = []\n",
    "for (dirpath, dirnames, filenames) in walk(dailypath):\n",
    "    dailyfiles.extend(filenames)\n",
    "    break\n",
    "for f in hourlyfiles:\n",
    "    date_parser = pd.to_datetime\n",
    "    dtypes = {'MAC':'str','PostIN KB':'int','PostOut KB':'int','PreIn KB':'int','PreOut KB':'int','LastSeen':'str'}\n",
    "    headers =  ['MAC','PostIN KB','PostOut KB','PreIn KB','PreOut KB','LastSeen']\n",
    "    parse_dates =  ['LastSeen']\n",
    "    if f[-3:] == 'dat' and f[0:6] == 'hourly':\n",
    "#         print(f)\n",
    "        df = pd.read_csv(hourlypath+f, names=headers, dtype=dtypes, parse_dates=parse_dates )\n",
    "        df['idx'] = idx\n",
    "        if idx == 0:\n",
    "            finaldf = df.copy()                 \n",
    "        else:\n",
    "            finaldf = pd.concat([finaldf,df])\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After successful download, consider cleaning out the remote directory\n",
    "\n",
    "```\n",
    "ssh -i ~/.ssh/router root@192.168.XXX.XXX\n",
    "\n",
    "nvram show | grep size\n",
    "\n",
    "cd /var/bwmon/data\n",
    "rm <<files>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation of data, final prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#calculate the totalGB column. This is approximate but very close...\n",
    "finaldf['totalGB'] = (finaldf['PostIN KB'] + finaldf['PostOut KB'])/1e6\n",
    "finaldf['dtstamp'] = finaldf['LastSeen'].dt.date\n",
    "finaldf['hour'] = finaldf['LastSeen'].dt.hour\n",
    "finaldf['year'] = finaldf['LastSeen'].dt.year\n",
    "finaldf['day'] = finaldf['LastSeen'].dt.day\n",
    "finaldf['month'] = finaldf['LastSeen'].dt.month\n",
    "finaldf['weekday'] = finaldf['LastSeen'].dt.weekday_name\n",
    "\n",
    "days ={0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:\"Sun\"}\n",
    "finaldf['dow'] = finaldf['LastSeen'].dt.weekday.map(days)\n",
    "\n",
    "\n",
    "pb = finaldf.MAC.nunique()\n",
    "#join with devicenames\n",
    "finaldf = finaldf.merge(macdf, left_on='MAC', right_on='MAC', how='left')\n",
    "#Make sure unknown devices are not ignored in the groupby by replacing the null devicename with\n",
    "# the mac address\n",
    "finaldf.devicename.fillna(finaldf['MAC'], inplace=True)\n",
    "pa = finaldf.MAC.nunique()\n",
    "print(\"Quality Check: \" + str(pb) + \" \" + str(pa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = finaldf[['MAC','LastSeen','dtstamp','year','month','day','hour','dow','weekday','totalGB','devicename']]\n",
    "finaldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropouts - get list of MACs that havent been on the network for awhile:\n",
    "#finaldf['legacy'] =\n",
    "print('This is a list of devices not seen in the past 30 days:')\n",
    "drops = finaldf.groupby(['MAC','devicename'])['LastSeen'].max()< datetime.datetime.now() - datetime.timedelta(days=30)\n",
    "drops[drops == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate some values for determining temporal location\n",
    "dyear = datetime.datetime.now().year \n",
    "dmonth =datetime.datetime.now().month - 2\n",
    "if dmonth <= 0:\n",
    "    dyear += -1\n",
    "    dmonth = 12 + dmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create summaries\n",
    "Alltotal = finaldf.groupby(['MAC','devicename'])['totalGB'].sum()\n",
    "all_final = finaldf\n",
    "devicetotal = finaldf.groupby(['devicename'])['totalGB'].sum()\n",
    "dailytotal = finaldf.groupby(['dtstamp'])['totalGB'].sum()\n",
    "smalldaily = finaldf[finaldf.totalGB < cutoff].groupby(['dtstamp','devicename'])['totalGB'].sum()\n",
    "\n",
    "if datetime.datetime.now().month == 1:\n",
    "    lastK = datetime.date(datetime.datetime.now().year-1, 12 , 28)\n",
    "elif datetime.datetime.now().day > 28:\n",
    "    lastK = datetime.date(datetime.datetime.now().year,datetime.datetime.now().month, 28)\n",
    "else:\n",
    "    lastK = datetime.date(datetime.datetime.now().year,datetime.datetime.now().month -1 , 28)\n",
    "print(lastK)\n",
    "recentdf = finaldf[finaldf['dtstamp'] > lastK]\n",
    "#this is empty until we get past Apr 28, 2018\n",
    "nonrecentdf = finaldf[finaldf['dtstamp'] <= lastK]\n",
    "print(len(recentdf))\n",
    "print(len(nonrecentdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results\n",
    "## Billing cycle ends on the 28th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate usage since last month end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldaily = pd.read_pickle('/Users/samuelcroker/Documents/routerdata/legacy.p').append(dailytotal)\n",
    "alldaily.index = pd.to_datetime(alldaily.index)\n",
    "plt.rcParams['figure.figsize'] = 16, 8\n",
    "calmap.yearplot(data=alldaily,year=2018,cmap='Greens',alpha=.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#integrate legacy data from previous version of this analysis.\n",
    "# note this data does not contain hourly results.\n",
    "\n",
    "legacydf = pd.read_pickle('/Users/samuelcroker/Documents/routerdata/legacyDaily.p')\n",
    "final_legacy = legacydf.append(nonrecentdf)\n",
    "\n",
    "# data quality check\n",
    "flu = pd.DataFrame(final_legacy.devicename.unique())\n",
    "fru = pd.DataFrame(recentdf.devicename.unique())\n",
    "flu.columns = ['LegacyDevice']\n",
    "fru.columns = ['RecentDevice']\n",
    "\n",
    "#uncomment below to check\n",
    "#flu.merge(fru,left_on='LegacyDevice',right_on='RecentDevice',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stil need to add recent legacy (dont have until apr 28)\n",
    "# Eventually, we can replace this legacy code with the monthly average \n",
    "# over devices rather and it will give a better picture of the month to month.\n",
    "# we should also add a power-company-like graph that shows the month\n",
    "# over month change by heavy utilizing devices, or devices that have\n",
    "# a big month to month util change - the AVG scenario\n",
    "\n",
    "all_device = finaldf.groupby(['devicename'])['totalGB'].sum()\n",
    "recent_device = recentdf.groupby(['devicename'])['totalGB'].sum()\n",
    "nonrecent_device = nonrecentdf.groupby(['devicename'])['totalGB'].sum()\n",
    "\n",
    "all_device.sort_values(ascending=False,inplace=True)\n",
    "recent_device.sort_values(ascending=False,inplace=True)\n",
    "nonrecent_device.sort_values(ascending=False,inplace=True)\n",
    "\n",
    "# legacy_device = legacydf.groupby(['devicename'])['totalGB'].sum()\n",
    "# legacy_device.sort_values(ascending=False,inplace=True)\n",
    "\n",
    "# za =  pd.DataFrame(recent_device)\n",
    "# # zb =  pd.DataFrame(legacy_device)\n",
    "# zc = pd.DataFrame(nonrecent_device)\n",
    "# za['device'] = za.index\n",
    "# # zb['device'] = zb.index\n",
    "# zc['device'] = zc.index\n",
    "\n",
    "# zap = za.merge(zc, how='outer',left_on='device',right_on='device')\n",
    "# # zbp = zb.merge(za, how='outer',left_on='device',right_on='device')\n",
    "\n",
    "# zap.totalGB_x.fillna(zap.totalGB_y,inplace=True)\n",
    "# zaps = pd.Series(zap['totalGB_y'].values,index=zap['device'])\n",
    "# zaps.sort_values(ascending=False,inplace=True)\n",
    "\n",
    "col_ord = all_device.index.tolist()\n",
    "# zaps = zaps.reindex(index=col_ord)\n",
    "\n",
    "# zbps = pd.Series(zbp['totalGB_x'].values,index=zbp['device'])\n",
    "\n",
    "# legacy_device = zbps.reindex(index=col_ord)\n",
    "recent_device = recent_device.reindex(index=col_ord)\n",
    "nonrecent_device= nonrecent_device.reindex(index=col_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List devices and usage\n",
    "This graph shows previous usage as orange bacground and current as blue foreground. I think there is a problem that will have to be looked at as the previous legacy data should be a monthly max rather than a sum. Maybe it is alredy doing that - but take a look anyway.\n",
    "\n",
    "what this shows is what devices are seeing a increase or decrease in usage, and since the graph is in logarithmic scale on the y axis, both high and low utilizers are seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tt is the tick marks and tlab is the associated labels. I put them like this for readability\n",
    "tt = [0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "tlab = ['100KB','1MB','10MB','100MB','1GB','10GB','100GB','1TB']\n",
    "\n",
    "pp0 = nonrecent_device.plot(kind='bar',color='darkorange',alpha=.35,stacked=False, figsize=(12, 5),logy=True,yticks=tt)\n",
    "pp0 = recent_device.plot(kind='bar',color='dodgerblue',alpha=0.45,stacked=False, figsize=(12, 5),logy=True,yticks=tt)\n",
    "pp0.set_yscale = 'symlog'\n",
    "pp0.set_yticklabels(tlab)\n",
    "pp0.set_xlabel('Date')\n",
    "pp0.set_ylabel('GB (Log Scale)')\n",
    "pp0.set_title(\"All Devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy Daily Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonrecentdaily = nonrecentdf.groupby(['dtstamp','weekday'])['totalGB'].sum()\n",
    "pp0 = nonrecentdaily.plot(kind='bar',stacked=False, color='darkorange',alpha=0.25,figsize=(12, 5))\n",
    "pp0.set_xlabel('Date')\n",
    "\n",
    "pp0.set_ylabel('GB')\n",
    "pp0.set_title(\"Current Billing Cycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Daily Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recentdaily = recentdf.groupby(['dtstamp','weekday'])['totalGB'].sum()\n",
    "pp0 = recentdaily.plot(kind='bar',stacked=False, color='dodgerblue',alpha=0.25,figsize=(12, 5))\n",
    "pp0.set_xlabel('Date')\n",
    "pp0.set_ylabel('GB')\n",
    "pp0.set_title(\"Current Billing Cycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progression Towards 1TB Data Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recentDF = pd.DataFrame(recentdf.groupby(['dtstamp'])['totalGB'].sum())\n",
    "recentDF['csum'] = recentDF['totalGB'].cumsum()\n",
    "\n",
    "tt = [0.0001,0.001,0.01,0.1,1,10,100,1000,2048]\n",
    "tlab = ['100KB','1MB','10MB','100MB','1GB','10GB','100GB','1TB']\n",
    "\n",
    "# pp0 = recentDF.plot(kind='bar',stacked=False, alpha=0.25,figsize=(12, 5),logy=True,yticks=tt)\n",
    "# pp0.set_yscale = 'symlog'\n",
    "# pp0.set_yticklabels(tlab)\n",
    "pp0 = recentDF.plot(kind='bar',stacked=False, alpha=0.25,figsize=(12, 5))\n",
    "pp0.set_xlabel('Date')\n",
    "pp0.axhline(y=1024,linewidth=1, color='r')\n",
    "pp0.set_ylabel('GB')\n",
    "pp0.set_title(\"Current Billing Cycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Utilization\n",
    "### Shows legacy vs current billing cycle \n",
    "Orange background = Legacy\n",
    "Blue foreground = Current billing cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlyall = finaldf.groupby(['hour','month'])['totalGB'].sum().groupby(level=[0]).mean().sort_index()\n",
    "hourlycurrent = recentdf.groupby(['hour'])['totalGB'].sum().sort_index()\n",
    "\n",
    "pp0 = hourlyall.plot(kind='bar',stacked=False, color='darkorange',alpha=0.25,figsize=(12, 5))\n",
    "pp0 = hourlycurrent.plot(kind='bar',stacked=False, color='dodgerblue',alpha=0.45,figsize=(12, 5))\n",
    "#overlay current billing cycle\n",
    "pp0.set_xlabel('Hour')\n",
    "pp0.set_ylabel('GB')\n",
    "pp0.set_title(\"Current Billing Cycle - Hourly Usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Hourly Heatmap\n",
    "This graph is useful for finding devices that are being utilized at odd times. For example, I would not want to see one of the kids iPads with high utilization during the middle of the night potentially indicating unsupervised YouTube usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "hourorder =[2,3,4,5,6,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,22,23,0,1]\n",
    "\n",
    "finaldf['totalMB'] = finaldf['totalGB']*1000\n",
    "finaldf['totalMBlog'] = finaldf['totalMB'].apply(lambda x: math.log(x))\n",
    "devicehour = pd.pivot_table(data=finaldf,index='hour',values='totalMB',columns='devicename',aggfunc='mean')#,fill_value=0)\n",
    "\n",
    "plt.subplots(figsize=(16, 7))\n",
    "devicehour=devicehour.reindex(index=hourorder)\n",
    "sns.heatmap(devicehour, cmap='Blues',annot=True,fmt=\".0f\",annot_kws={'size':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print a table of this graph - uncomment for use\n",
    "# devicehour.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largemac = devicehour[devicehour.columns.intersection(devicehour.sum().nlargest(5).index.tolist())]\n",
    "largemac = largemac.reindex(index=hourorder)\n",
    "plt.subplots(figsize=(9, 7))\n",
    "sns.heatmap(largemac, cmap='Blues',annot=True,fmt=\".0f\",annot_kws={'size':8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day of Week by Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\n",
    "dayhour = pd.pivot_table(data=finaldf,index='dow',values='totalMB',columns='hour',aggfunc='mean')#,fill_value=0)\n",
    "dayhour = dayhour.reindex(weekdays).transpose()\n",
    "dayhour = dayhour.reindex(index=hourorder)\n",
    "plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(dayhour, cmap='Blues',annot=True,fmt=\".0f\",annot_kws={'size':8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Lost macs\n",
    "# # 5c:8d:4e:d9:7c:f8\n",
    "# # f0:a2:25:f0:e4:60 (only 1 occurrance - Sunday 4-22)\n",
    "# # e4:e4:ab:bf:39:df Pretty frequent\n",
    "# finaldf[finaldf['MAC']=='e4:e4:ab:bf:39:df'].sort_values(by='LastSeen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3.5]",
   "language": "python",
   "name": "conda-env-Python3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
